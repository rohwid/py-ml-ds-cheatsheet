{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning adalah suatu teknik atau metode yang memanfaatkan model yang sudah dilatih terhadap suatu dataset untuk menyelesaikan permasalahan lain yang serupa dengan cara menggunakannya sebagai starting point, memodifikasi dan mengupdate parameternya sehingga sesuai dengan dataset yang baru."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet\n",
    "\n",
    "<img src=\"img/imageNet.jpg\">\n",
    "\n",
    "Sebelum melangkah lebih lanjut saya akan bahas sedikit tentang ImageNet. ImageNet adalah sebuah dataset yang terdiri dari 1.200.000 gambar untuk training dan 100.000 untuk testing. Dataset ini terdiri dari 1000 classes jadi untuk setiap class ada 1.200 gambar.\n",
    "\n",
    "Tiap tahun ada semacam challenge untuk mencari model buatan siapa yang mempunyai tingkat akurasi tertinggi.\n",
    "\n",
    "<img src=\"img/imageNet_result.png\">\n",
    "\n",
    "Sebenarnya ImageNet challenge ini sudah dimulai sejak 2010, saya kurang paham algoritma dan model apa yang digunakan pada rentang 2010â€“2011. Namun hasil yang paling standout adalah AlexNet pada 2012. AlexNet adalah model pertama yang menggunakan Convolutional Neural Network (CNN). Untuk detail lebih lanjut mungkin bisa dibaca sendiri papernya di [sini](https://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf).\n",
    "\n",
    "<img src=\"img/more_deeper_better.png\">\n",
    "\n",
    "Seiring dengan perkembangan teknologi tiap tahun, jumlah layer yang digunakan juga mengalami kenaikan yang hasilnya bisa dibilang setara dengan tingkat akurasi yang dihasilkan.\n",
    "\n",
    "<img src=\"img/alexNet_architecture.png\">\n",
    "\n",
    "Gambar diatas adalah ilustrasi arsitektur AlexNet. Sama seperti arsitektur model yang sudah kita coba pada Part-7 lalu. AlexNet terbagi menjadi 2 bagian yaitu Feature Extraction layer dan Fully-Connected Layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification\n",
    "\n",
    "Kali ini kita akan melakukan percobaan untuk melakukan klasifikasi jenis kelamin seseorang berdasarkan foto mereka.\n",
    "\n",
    "Dataset yang digunakan saya kumpulkan dari [UI Faces API](https://uifaces.co/api-docs) dan [Random User Generator](https://randomuser.me/photos). Ada 800 foto untuk training data dan 240 foto untuk testing yang dibagi menjadi 2 classes yaitu male dan female. Gambar dari Random User Generator mempunyai ukuran yang sama yaitu 128x128 pixels namun foto dari UI Faces mempunyai ukuran yang berbeda-beda. Sehingga kita harus melakukan pre-processing terlebih dahulu untuk mendapatkan ukuran foto yang seragam sehingga dapat lebih mudah saat melakukan training.\n",
    "\n",
    "[Download Dataset](https://www.dropbox.com/s/9e1ix1xunqlq6vw/dataset.zip?dl=0)\n",
    "\n",
    "<img src=\"img/sample_dataset.png\">\n",
    "\n",
    "Pada percobaan ini kita akan menggunakan 2 buah model. Model pertama kita akan menggunakan arsitektur yang sederhana yang kita kasih nama KorNet dan model kedua adalah VGG-16 Network. Buat yang penasaran tentang VGG secara lengkap, bisa dibaca sendiri papernya di [sini](https://arxiv.org/pdf/1409.1556.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KorNet\n",
    "\n",
    "Arsitektur sederhana kita ini terdiri dari 87.969 buah parameter yang akan diupdate pada saat training. Pada *feature learning layer* terdapat **4 Convolution Layer**, **ZeroPadding Layer** dan **MaxPooling Layer**.\n",
    "\n",
    "Sedangkan pada *fully-connected layer* terdapat **2 buah layer dengan jumlah neuron masing-masing sebanyak 32 dan 1**. Perlu diingat bahwa layer terakhir adalah output layer. **1 buah layer disini karena kita akan melakukan binary classification, 0 untuk pria dan 1 untuk wanita**. Sehingga **activation function** yang harus kita gunakan pada output layer adalah **sigmoid dengan loss function binary crossentropy**. Kita juga bisa menggunakan 2 neuron pada output layer, menggunakan activation function softmax dan loss function categorical crossentropy seperti pada Part-7.\n",
    "\n",
    "Boleh dicoba mana yang lebih baik, tapi untuk percobaan kali ini, kita mau coba dengan 1 neuron dan sigmoid function. Berikut adalah contoh arsitektur korNet:\n",
    "\n",
    "<img src=\"img/cnn_korNet.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Global Variable\n",
    "\n",
    "Dependencies yang akan kita gunakan hampir sama dengan Part-7. Tapi kali ini kita akan gunakan **ImageDataGenerator**. Dimensi gambar yang digunakan adalah **128 x 128 pixels**. Kita juga gunakan TensorBoard untuk visualisasi pada saat training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Dense, Conv2D, MaxPooling2D, ZeroPadding2D, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Images Dimensions\n",
    "img_width, img_height = 128, 128\n",
    "\n",
    "train_data_dir = 'data/datasets/train'\n",
    "validation_data_dir = 'data/datasets/validation'\n",
    "nb_train_samples = 800\n",
    "nb_validation_samples = 240\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# TensorBoard Callbacks\n",
    "callbacks = TensorBoard(log_dir='./graph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "<img src=\"img/data_augmentation.png\">\n",
    "\n",
    "Seperti yang sudah kita ketahui, untuk mendapatkan performa yang optimal, Deep Learning membutuhkan data yang lebih banyak dibandingkan dengan algoritma ML yang lain.\n",
    "\n",
    "Dari dataset yang telah kita kumpulkan hanya terdapat 400 foto pria dan 400 foto wanita. Jumlah data tersebut masih kurang mencukupi untuk mendapatkan performa yang optimal.\n",
    "\n",
    "Untuk itu kita perlu meng-augmentasi data tersebut. Data Augmentation adalah sebuah teknik memanipulasi sebuah data tanpa kehilangan inti atau esensi dari data tersebut. Untuk data berupa Image, hal yang biasanya dilakukan adalah **rotate**, **flip**, **crop**, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 240 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training Data Augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# Rescale Testing Data\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# Train Data Generator\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_width, img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "# Testing Data Generator\n",
    "validation_generator = test_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                        target_size=(img_width, img_height),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada percobaan kita kali ini, kita akan melakukan **shear**, **zoom** dan **flip** sedangkan parameter rescale yang kita gunakan adalah membagi nilai RGB dari 0-255 dengan 255, sehingga kita mendapatkan nilai RGB pada rentang 0-1. Untuk **data testing kita hanya melakukan rescale saja**.\n",
    "\n",
    "Method `flow_from_directory` dari `ImageDataGenerator` kita gunakan untuk mengubah data yang berupa \"raw image\" menjadi sebuah dataset yang akan kita gunakan untuk training dan testing, tentu saja dataset yang telah kita augmentasi tadi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KorNet Model\n",
    "\n",
    "Kali ini kita akan melakukan klasifikasi terhadap 2 class atau biasa disebut binary classification. 0 untuk class pertama dan 1 untuk class kedua. Jika dilihat dari karakteristik outputnya, kita bisa gunakan \"Sigmoid\" sebagai activation function pada output layer dan untuk semua hidden layer kita gunakan ReLU.\n",
    "\n",
    "Karena ini adalah **binary classification**, **loss function** yang kita gunakan adalah **binary_crossentropy** dan **Adam** sebagai *optimizer*-nya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 16)        1216      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 44, 44, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 32)        12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                18464     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 87,969\n",
      "Trainable params: 87,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction Layer\n",
    "inputs = Input(shape=(img_width, img_height, 3))\n",
    "conv_layer = Conv2D(16, (5, 5), strides=(3,3), activation='relu')(inputs) \n",
    "conv_layer = ZeroPadding2D(padding=(1,1))(conv_layer) \n",
    "conv_layer = Conv2D(32, (5, 5), strides=(3,3), activation='relu')(conv_layer) \n",
    "conv_layer = MaxPooling2D((2, 2))(conv_layer) \n",
    "conv_layer = Conv2D(64, (3, 3), strides=(1,1), activation='relu')(conv_layer) \n",
    "conv_layer = Conv2D(64, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "\n",
    "# Flatten Layer\n",
    "flatten = Flatten()(conv_layer) \n",
    "\n",
    "# Fully Connected Layer\n",
    "fc_layer = Dense(32, activation='relu')(flatten)\n",
    "outputs = Dense(1, activation='sigmoid')(fc_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Adam Optimizer and Cross Entropy Loss\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print Model Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6919 - accuracy: 0.5200 - val_loss: 0.6878 - val_accuracy: 0.5958\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 6s 127ms/step - loss: 0.6871 - accuracy: 0.5387 - val_loss: 0.6895 - val_accuracy: 0.5833\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 6s 111ms/step - loss: 0.6686 - accuracy: 0.6325 - val_loss: 0.6303 - val_accuracy: 0.6417\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.6276 - accuracy: 0.6687 - val_loss: 0.5526 - val_accuracy: 0.6333\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.6086 - accuracy: 0.6737 - val_loss: 0.5375 - val_accuracy: 0.6375\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.5815 - accuracy: 0.7038 - val_loss: 0.6615 - val_accuracy: 0.6750\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 0.5492 - accuracy: 0.7150 - val_loss: 0.5147 - val_accuracy: 0.6333\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 0.5483 - accuracy: 0.7237 - val_loss: 0.5849 - val_accuracy: 0.7250\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 0.5244 - accuracy: 0.7487 - val_loss: 0.3722 - val_accuracy: 0.7458\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 0.5243 - accuracy: 0.7475 - val_loss: 0.6890 - val_accuracy: 0.7375\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.5006 - accuracy: 0.7812 - val_loss: 0.3685 - val_accuracy: 0.7625\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 0.4857 - accuracy: 0.7788 - val_loss: 0.7293 - val_accuracy: 0.7542\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.5037 - accuracy: 0.7600 - val_loss: 0.4403 - val_accuracy: 0.7417\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.4735 - accuracy: 0.7800 - val_loss: 0.6368 - val_accuracy: 0.7583\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.4761 - accuracy: 0.7638 - val_loss: 0.5155 - val_accuracy: 0.7500\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.4487 - accuracy: 0.7937 - val_loss: 0.4069 - val_accuracy: 0.7500\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.4494 - accuracy: 0.8037 - val_loss: 0.6476 - val_accuracy: 0.7625\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 0.4395 - accuracy: 0.7975 - val_loss: 0.3221 - val_accuracy: 0.7125\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.4437 - accuracy: 0.7912 - val_loss: 0.3342 - val_accuracy: 0.7583\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.4137 - accuracy: 0.8100 - val_loss: 0.6004 - val_accuracy: 0.7750\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 0.4014 - accuracy: 0.8175 - val_loss: 0.4205 - val_accuracy: 0.7500\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.4278 - accuracy: 0.8087 - val_loss: 0.4352 - val_accuracy: 0.7750\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.4141 - accuracy: 0.8225 - val_loss: 0.5482 - val_accuracy: 0.7875\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.3969 - accuracy: 0.8075 - val_loss: 0.6252 - val_accuracy: 0.7500\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 0.3648 - accuracy: 0.8537 - val_loss: 0.7373 - val_accuracy: 0.7292\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 0.3878 - accuracy: 0.8275 - val_loss: 0.3190 - val_accuracy: 0.7958\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.3778 - accuracy: 0.8188 - val_loss: 0.6124 - val_accuracy: 0.7958\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.3612 - accuracy: 0.8525 - val_loss: 0.6678 - val_accuracy: 0.7750\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.3613 - accuracy: 0.8438 - val_loss: 0.4466 - val_accuracy: 0.7500\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.3751 - accuracy: 0.8388 - val_loss: 0.5205 - val_accuracy: 0.7750\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.3501 - accuracy: 0.8475 - val_loss: 0.4713 - val_accuracy: 0.7792\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.3458 - accuracy: 0.8512 - val_loss: 0.3480 - val_accuracy: 0.7625\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 0.3467 - accuracy: 0.8600 - val_loss: 0.5213 - val_accuracy: 0.7917\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 0.3343 - accuracy: 0.8687 - val_loss: 0.5148 - val_accuracy: 0.7833\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 0.3473 - accuracy: 0.8413 - val_loss: 0.4598 - val_accuracy: 0.8125\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 0.3240 - accuracy: 0.8625 - val_loss: 0.2949 - val_accuracy: 0.7958\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.3156 - accuracy: 0.8750 - val_loss: 0.1973 - val_accuracy: 0.8125\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 0.3237 - accuracy: 0.8500 - val_loss: 0.3696 - val_accuracy: 0.7792\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 0.3058 - accuracy: 0.8725 - val_loss: 0.7010 - val_accuracy: 0.7917\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.2999 - accuracy: 0.8788 - val_loss: 0.1455 - val_accuracy: 0.8208\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.2777 - accuracy: 0.8863 - val_loss: 0.2647 - val_accuracy: 0.7958\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 0.2976 - accuracy: 0.8625 - val_loss: 0.2073 - val_accuracy: 0.8250\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.2864 - accuracy: 0.8838 - val_loss: 0.3801 - val_accuracy: 0.8167\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.2773 - accuracy: 0.8925 - val_loss: 0.3766 - val_accuracy: 0.8083\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.2879 - accuracy: 0.8863 - val_loss: 0.3775 - val_accuracy: 0.8250\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.2687 - accuracy: 0.8913 - val_loss: 0.3444 - val_accuracy: 0.8083\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 0.2663 - accuracy: 0.8938 - val_loss: 0.3709 - val_accuracy: 0.8167\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 0.2766 - accuracy: 0.8913 - val_loss: 0.1665 - val_accuracy: 0.7875\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.2680 - accuracy: 0.8925 - val_loss: 0.3924 - val_accuracy: 0.7917\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 0.2568 - accuracy: 0.9025 - val_loss: 0.6827 - val_accuracy: 0.8042\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size, \n",
    "                    callbacks=[callbacks])\n",
    "\n",
    "model.save_weights('kornet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah 50 epoch training-testing, kita mendapatkan loss dan accuracy sebesar 0.4847 crossentropy loss dan 76.97% accuracy. Grafik dibawah juga menunjukkan bahwa performa dari model kita ini tidak begitu baik. Terdapat indikasi overfitting, nilai loss dan akurasi yang relatif rendah.\n",
    "\n",
    "<img src=\"img/tf_board_kornet.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
