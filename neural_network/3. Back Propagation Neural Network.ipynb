{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation Neural Network\n",
    "\n",
    "Little bit mention from the previous section about how to do training on neural networks. The training process consists of 2 main parts, that named the **Forward Pass** and **Backward Pass**. The blue arrow below is the **Forward Pass** and the red arrow is the **Backward Pass**.\n",
    "\n",
    "<img src=\"img/nn_training.png\">\n",
    "\n",
    "In Supervised Learning, training data consists of input and output/target. At the **Forward Pass**, the input will be `\"propagated\"` to the output layer and the predicted output will be compared to the target by using a function, that commonly called the **Loss Function**.\n",
    "\n",
    "Then what is the loss function for? In simple **Loss Function** is used to measure how well the performance of our neural network in predicting targets.\n",
    "\n",
    "$Loss = (Target - Prediction)^2$\n",
    "\n",
    "There are various types of loss functions, but the most commonly used was Squared Error (L2 Loss) for regression. And for the classification commonly used was Cross Entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Backward Pass (Back-Propagation)\n",
    "\n",
    "The simplicity of this process is to adjust each weight and bias based on the errors obtained at the forward pass. The stages of backprop are as follows:\n",
    "\n",
    "- Calculate the gradient of the loss function for all parameters by finding the partial derivative of the function. Here we can use the **Chain Rule method**. For those who are still confused about what gradient is, maybe the illustration below can help.\n",
    "\n",
    "<img src=\"img/gradient_descent.gif\">\n",
    "\n",
    "- Update all parameters (weight and bias) using the **Stochastic Gradient Descent (SGD)** by subtracting or adding the old weight value to the \"partial\" (***learning rate***) of the gradient value that got.\n",
    "\n",
    "<img src=\"img/nn_bp.png\">\n",
    "\n",
    "The neural network above consists of **2 hidden layers**. The first hidden layer uses **ReLU**, the second hidden layer uses sigmoid and finally the output layer uses linear as the activation function. The bias in the diagram above actually exists but is not illustrated.\n",
    "\n",
    "There are **4 weights** and **4 biases** between the input layer and the **first hidden layer**, **8 weights** and **2 biases** between the **first and second hidden layers**, **2 weights** and **1 bias** between the second hidden layer and the output layer. So that in total there are **21 parameters** that must be updated.\n",
    "\n",
    "Here to predict a value with the input and output like this.\n",
    "\n",
    "$input = [2.0]$\n",
    "\n",
    "$output = [3.0]$\n",
    "\n",
    "For the initial weight and bias, The values was determined with the values that can calculate easily.\n",
    "\n",
    "- **Weight**\n",
    "\n",
    "$W_{jk} = \\begin{bmatrix}w_{ij_{1}} & w_{ij_{2}} & w_{ij_{3}} & w_{ij_{4}} \\end{bmatrix} = \\begin{bmatrix}0.25 & 0.5 & 0.75 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$W_{jk} = \\begin{bmatrix}\n",
    "w_{j_{1}k_{1}} & w_{j_{1}k_{2}} \\\\\n",
    "w_{j_{2}k_{1}} & w_{j_{1}k_{2}} \\\\\n",
    "w_{j_{3}k_{1}} & w_{j_{3}k_{2}} \\\\\n",
    "w_{j_{4}k_{1}} & w_{j_{4}k_{2}}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1.0 & 0 \\\\\n",
    "0.75 & 0.25 \\\\\n",
    "0.5 & 0.5 \\\\\n",
    "0.25 & 0.75\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$W_{ko} = \\begin{bmatrix}w_{k_{1}o} \\\\ w_{k_{2}o} \\end{bmatrix} = \\begin{bmatrix}1.0 \\\\ 0.5 \\end{bmatrix}$\n",
    "\n",
    "- **Bias**\n",
    "\n",
    "$b_{ij} = \\begin{bmatrix}b_{ij_{1}} & b_{ij_{2}} & b_{ij_{3}} & b_{ij_{4}} \\end{bmatrix} = \\begin{bmatrix} 1.0 & 1.0 & 1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$b_{jk} = \\begin{bmatrix}b_{jk_{1}} & b_{jk_{2}} \\end{bmatrix} = \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$b_{o} = \\begin{bmatrix} 1.0 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input to Hidden Layer 1 (Forward Passs)\n",
    "\n",
    "<img src=\"img/nn_input_hidden_layer_1.png\">\n",
    "\n",
    "Here the input data will forward to the hidden layer 1. Then multiply (dot product) and adding the matrix between inputs, weights and biases.\n",
    "\n",
    "$\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix} = \\begin{bmatrix}input \\end{bmatrix} \\times \\begin{bmatrix}W_{ij_{1}} & W_{ij_{2}} & W_{ij_{3}} & W_{ij_{4}} \\end{bmatrix} + \\begin{bmatrix}b_{ij_{1}} & b_{ij_{2}} & b_{ij_{3}} & b_{ij_{4}} \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix} = \\begin{bmatrix}2.0 \\end{bmatrix} \\times \\begin{bmatrix}0.25 & 0.5 & 0.75 & 1.0 \\end{bmatrix} + \\begin{bmatrix}1.0 & 1.0 & 1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix} = \\begin{bmatrix}2.0 \\end{bmatrix} \\times \\begin{bmatrix}0.25 & 0.5 & 0.75 & 1.0 \\end{bmatrix} + \\begin{bmatrix}1.0 & 1.0 & 1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix} = \\begin{bmatrix}0.5 & 1.0 & 1.5 & 2.0 \\end{bmatrix} + \\begin{bmatrix}1.0 & 1.0 & 1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix} = \\begin{bmatrix}1.5 & 2.0 & 2.5 & 3.0 \\end{bmatrix}$\n",
    "\n",
    "The values above was input from each node in hidden layer 1. All values will be issued after going through the activation function. On the hidden layer 1 activation function that used was **ReLU** $\\rightarrow f(x) = max(0, x)$. So the output of hidden layer 1 will be like this:\n",
    "\n",
    "$ReLU\\left(\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix}\\right) = \\begin{bmatrix}max(0, j1_{in}) & max(0, j2_{in}) & max(0, j3_{in}) & max(0, j4_{in}) \\end{bmatrix}$\n",
    "\n",
    "$ReLU\\left(\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix}\\right) = \\begin{bmatrix}max(0, 1.5) & max(0, 2.0) & max(0, 2.5) & max(0, 3.0) \\end{bmatrix}$\n",
    "\n",
    "$ReLU\\left(\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix}\\right) = \\begin{bmatrix}1.5 & 2.0 & 2.5 & 3.0 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix} = \\begin{bmatrix}1.5 & 2.0 & 2.5 & 3.0 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer 1 to Hidden Layer 2 (Forward Pass)\n",
    "\n",
    "<img src=\"img/nn_hidden_layer_1_hidden_layer_2.png\">\n",
    "\n",
    "Just like the **Forward Pass** on the previous layer, the output of each neuron in the **ReLU** layer will flow to all neurons in the **Sigmoid** layer.\n",
    "\n",
    "$\\begin{bmatrix} k_{1in} & k_{2in}\\end{bmatrix} = \\begin{bmatrix}j1_{in} & j2_{in} & j3_{in} & j4_{in} \\end{bmatrix} \\times \\begin{bmatrix} \n",
    "w_{j_{1}k_{1}} & w_{j_{1}k_{2}} \\\\\n",
    "w_{j_{2}k_{1}} & w_{j_{2}k_{2}} \\\\\n",
    "w_{j_{3}k_{1}} & w_{j_{3}k_{2}} \\\\\n",
    "w_{j_{4}k_{1}} & w_{j_{4}k_{2}}\n",
    "\\end{bmatrix} + \\begin{bmatrix}b_{jk_{1}} & b_{jk_{2}}\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} k_{1in} & k_{2in}\\end{bmatrix} = \\begin{bmatrix}1.5 & 2 & 2.5 & 3.0 \\end{bmatrix} \\times \\begin{bmatrix} \n",
    "1.0 & 0 \\\\\n",
    "0.75 & 0.25 \\\\\n",
    "0.5 & 0.5 \\\\\n",
    "0.25 & 0.75\n",
    "\\end{bmatrix} + \\begin{bmatrix}1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} k_{1in} & k_{2in}\\end{bmatrix} = \\begin{bmatrix} 5.0 & 4.0 \\end{bmatrix} + \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} k_{1in} & k_{2in}\\end{bmatrix} = \\begin{bmatrix} 6.0 & 5.0 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After activation function (**Sigmoid**):\n",
    "\n",
    "$Sigmoid \\rightarrow f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$Sigmoid \\left(\\begin{bmatrix}k_{1in} & k_{2in} \\end{bmatrix}\\right) = \\begin{bmatrix}\\frac{1}{1 + e^{-k_{1in}}}\\ \\frac{1}{1 + e^{-k_{1in}}} \\end{bmatrix}$\n",
    "\n",
    "$Sigmoid \\left(\\begin{bmatrix}k_{1in} & k_{2in} \\end{bmatrix}\\right) = \\begin{bmatrix}\\frac{1}{1 + e^{-6}} & \\frac{1}{1 + e^{-5}} \\end{bmatrix}$\n",
    "\n",
    "$Sigmoid \\left(\\begin{bmatrix}k_{1in} & k_{2in} \\end{bmatrix}\\right) = \\begin{bmatrix}0.9975 & 0.9933 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}k_{1in} & k_{2in} \\end{bmatrix} = \\begin{bmatrix}0.9975 & 0.9933 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer 2 to Output (Forward Pass)\n",
    "\n",
    "<img src=\"img/nn_hidden_layer_2_output.png\">\n",
    "\n",
    "Just like the **Forward Pass** on the previous layer, the output of each neuron in the **Sigmoid** Layer will flow to the neurons in the **Linear** layer (Output).\n",
    "\n",
    "$\\begin{bmatrix}o_{in}\\end{bmatrix} = \\begin{bmatrix}k_{1out} & k_{2out}\\end{bmatrix} \\times \\begin{bmatrix}\n",
    "w_{k_{1}o} \\\\ \n",
    "w_{k_{1}o} \n",
    "\\end{bmatrix} + \\begin{bmatrix}b_{o}\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}o_{in}\\end{bmatrix} = \\begin{bmatrix}0.9975 & 0.9933 \\end{bmatrix} \\times \\begin{bmatrix}\n",
    "1.0 \\\\ \n",
    "0.5 \n",
    "\\end{bmatrix} + \\begin{bmatrix}b_{o}\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}o_{in}\\end{bmatrix} = \\begin{bmatrix}1.494 \\end{bmatrix} + \\begin{bmatrix}1.0 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}o_{in}\\end{bmatrix} = \\begin{bmatrix}2.494 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After activation function (**Linear**)\n",
    "\n",
    "$Linear \\rightarrow f(x) = x$\n",
    "\n",
    "$f\\left(\\begin{bmatrix}o_{in} \\end{bmatrix}\\right) = \\begin{bmatrix}2.494 \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix}o_{out} \\end{bmatrix} = \\begin{bmatrix}2.494 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output prediction value had been gotten. The Next thing to do is looking for the _losses_ using **squared errors** (L2 Loss).\n",
    "\n",
    "$Loss = \\frac{1}{2}(Prediction - Target)^2$\n",
    "\n",
    "$Loss = \\frac{1}{2}(o_{out} - output)^2$\n",
    "\n",
    "$Loss = \\frac{1}{2}(2.494 - 3)^2$\n",
    "\n",
    "$Loss = \\frac{1}{2}(-0.506)^2$\n",
    "\n",
    "$Loss = \\frac{1}{2}(0.506)$\n",
    "\n",
    "$Loss = 0.128$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function Derivatives\n",
    "\n",
    "Before discussing the **Backward Pass**, it's a good idea to take a look first for the activation function that gonna be use.\n",
    "\n",
    "- **ReLu** Derivatives\n",
    "\n",
    "$y = max(0, x)$\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x} = \\begin{cases}1 & x > 0\\\\0 & x \\leq 0\\end{cases}$\n",
    "\n",
    "- **Sigmoid** Derivatives\n",
    "\n",
    "$y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x} = \\frac{1}{1 + e^{-x}} \\times \\left(1 - \\frac{1}{1 + e^{-x}}\\right)$\n",
    "\n",
    "- **Linear** Derivatives\n",
    "\n",
    "$y = x$\n",
    "\n",
    "$\\frac{\\partial y}{\\partial x} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to Hidden Layer 2 (Backward Pass)\n",
    "\n",
    "<img src=\"img/bp_output_hidden_layer_2.png\">\n",
    "\n",
    "Almost the same with the **Forward Pass**, on the **Backward Pass**, loss will flow to all the nodes in the hidden layer to find the _gradient_ and update the parameters. For the example to update the $w_{k_{1}o}$ parameter, then the **Chain Rule** which described below can be use.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{k_{1}o}} = \\frac{\\partial Loss}{\\partial o_{out}} \\times \\frac{\\partial o_{out}}{\\partial o_{in}} \\times \\frac{\\partial o_{in}}{\\partial w_{k_{1}o}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take a look about how much the ***Loss*** changes based on output. So we have to look for a ***partial derivative*** of the ***loss function*** to ***output***, it can also call as the ***gradient loss*** function of output. In the equation below, the ***Loss*** will be multiplied by 1/2, the actual main goal about why it needed to derivate was the ***Loss function*** set to produce 1 time ***Loss*** (neutralize the derivative of the quadratic function).\n",
    "\n",
    "- **Weight**\n",
    "\n",
    "$Loss = \\frac{1}{2} \\left(output - o_{out} \\right)^2$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial o_{out}} = \\frac{\\partial \\left(\\frac{1}{2} \\left(output - o_{out} \\right)^2 \\right)}{\\partial o_{out}}$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial o_{out}} = -1 \\times 2 \\times \\frac{1}{2} \\left(output - o_{out} \\right)$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial o_{out}} = o_{out} - output$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial o_{out}} = 2.494 - 3$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial o_{out}} = -0.506$\n",
    "\n",
    "- **Bias**\n",
    "\n",
    "$Loss = \\frac{1}{2} \\left(output - o_{out} \\right)^2$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial b_{o}} = \\frac{\\partial \\left(\\frac{1}{2} \\left(output - o_{out} \\right)^2 \\right)}{\\partial b_{o}}$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial o_{out}} = -1 \\times 2 \\times \\frac{1}{2} \\left(output - o_{out} \\right)$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial o_{out}} = -0.506$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look for the **gradient from $o_{out}$ to $o_{in}$**. Because the activation function used is Linear, its derivatives are very easy to find.\n",
    "\n",
    "$o_{out} = o_{in}$\n",
    "\n",
    "$\\frac{\\partial o_{out}}{\\partial o_{in}} = \\frac{\\partial (o_{in})}{\\partial o_{in}}$\n",
    "\n",
    "$\\frac{\\partial o_{out}}{\\partial o_{in}} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, the **gradient of $o_{in}$ toward $w_{k_{1}o}$, $w_{k_{2}o}$ and bias or** $b_{o}$ will be found. Consider the equation below:\n",
    "\n",
    "- **Weight**\n",
    "\n",
    "$o_{in} = w_{k_{1}o}k_{1out} + w_{k_{2}o}k_{2out} + b_{o}$\n",
    "\n",
    "$\\frac{\\partial\\ o_{in}}{\\partial\\ w_{k_{1}o}} = \\frac{\\partial\\ w_{k_{1}o}k_{1out} + w_{k_{2}o}k_{2out} + b_{o}}{\\partial\\ w_{k_{1}o}}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial o_{in}}{\\partial w_{k_{1}o}} \\\\ \n",
    "\\frac{\\partial o_{in}}{\\partial w_{k_{1}o}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "k_{1out} \\\\ \n",
    "k_{1out}\n",
    "\\end{bmatrix}  =\n",
    "\\begin{bmatrix}\n",
    "0.9975 \\\\ \n",
    "0.9933\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- **Bias**\n",
    "\n",
    "The **bias** value same with the **Forward Pass**.\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial o_{in}}{\\partial b_{o}}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "1.0\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, The chain rule to find the **Gradient loss for weight and bias** can be applied.\n",
    "\n",
    "- **Gradient loss** toward **weight** in Hidden Layer 2\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial w_{k_{1}o}} \\\\ \n",
    "\\frac{\\partial Loss}{\\partial w_{k_{2}o}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial Loss}{\\partial o_{out}} \\times \\frac{\\partial o_{out}}{\\partial o_{in}} \\times \\frac{\\partial o_{in}}{\\partial w_{k_{1}o}} \\\\ \n",
    "\\frac{\\partial Loss}{\\partial o_{out}} \\times \\frac{\\partial o_{out}}{\\partial o_{in}} \\times \\frac{\\partial o_{in}}{\\partial w_{k_{2}o}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial w_{k_{1}o}} \\\\ \n",
    "\\frac{\\partial Loss}{\\partial w_{k_{2}o}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "-0.506 \\times 1 \\times 0.9975 \\\\ \n",
    "-0.506 \\times 1 \\times 0.9933\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial w_{k_{1}o}} \\\\ \n",
    "\\frac{\\partial Loss}{\\partial w_{k_{2}o}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "-0.50474 \\\\ \n",
    "-0.50261\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- **Gradient loss** for **bias** ($b_{0}$) in Hidden Layer 2\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial b_{o}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial Loss}{\\partial b_{o}} \\times \\frac{\\partial o_{out}}{\\partial o_{in}} \\times \\frac{\\partial o_{in}}{\\partial b_{o}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial b_{o}}\n",
    "\\end{bmatrix} = \\left[-0.506 \\times 1 \\times 0.9975 \\right]$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial b_{o}}\n",
    "\\end{bmatrix} = \\left[ -0.506\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD) Update for Output to Hidden Layer 2\n",
    "\n",
    "**SGD** is an algorithm used to update parameters in this case Weight and bias. The algorithm is quite simple, basically the initial weight will be reduce by \"a portion\" of the gradient value that gotten.\n",
    "\n",
    "Some of this is represented by a hyper-parameter called learning rate ($\\alpha$). For example,  a learning rate set to 0.25 even though in practice learning rate 0.25 is not ideal. (Later will be discussed about setting *hyper-parameters*).\n",
    "\n",
    "- SGD update for **Weight** in Hidden Layer 2\n",
    "\n",
    "$w'_{k_{1}o} = w_{k_{1}o} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{k_{1}o}} \\right) = 1 - 0.25(-0.50474) = 1.1262$\n",
    "\n",
    "$w'_{k_{2}o} = w_{k_{2}o} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{k_{2}o}} \\right) = 1 - 0.25(-0.50261) = 0.6256$\n",
    "\n",
    "- SGD update for **Bias** in Hidden Layer 2\n",
    "\n",
    "$b'_{o} = b_{o} - \\alpha \\left(\\frac{\\partial Loss}{\\partial b_{o}} \\right) = 1 - 0.25(-0.506) = 1.1265$\n",
    "\n",
    "New parameters after update:\n",
    "\n",
    "$w_{ko} = \n",
    "\\begin{bmatrix} \n",
    "w_{k_{1}o} \\\\ \n",
    "w_{k_{2}o}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "1.1262 \\\\ \n",
    "0.6256\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$b_{o} = \\left[ 1.1265\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer 2 to Hidden Layer 1 (Backward Pass)\n",
    "\n",
    "<img src=\"img/bp_hidden_layer_2_hidden_layer_1.png\">\n",
    "\n",
    "Repeat every step that has been done on the previous layer (**Backward Pass**). But, in this step must be more careful when taking approach because it was relatively more complicated than the **Backward Pass** on the previous layer.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{j_{1}k_{1}}} = \\frac{\\partial Loss}{\\partial k_{1out}} \\times \\frac{\\partial k_{1out}}{\\partial k_{1in}} \\times \\frac{\\partial k_{1in}}{\\partial w_{j_{1}k_{1}}}$\n",
    "\n",
    "To find the **gradient loss** toward $w_{j_{1}k_{1}}$, the chain rule should be used again. First, gradient loss should be found using $k_{1out}$.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial k_{1out}} = \\frac{\\partial Loss}{\\partial o_{out}} \\times \\frac{\\partial o_{out}}{\\partial o_{in}} \\times \\frac{\\partial o_{in}}{\\partial w_{k_{1}o}} \\times \\frac{\\partial k_{1in}}{\\partial w_{j_{1}k_{1}}}$\n",
    "\n",
    "$...$\n",
    "\n",
    "$w_{ko} = \n",
    "\\begin{bmatrix} \n",
    "w_{k_{1}o} \\\\\n",
    "w_{k_{2}o}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "1.0 \\\\\n",
    "0.5\n",
    "\\end{bmatrix}, \\quad the\\ initial\\ weight$\n",
    "\n",
    "$...$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial k_{1out}} = 0.506 \\times 1 \\times 0.9975 \\times w_{k_{1}o}$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial k_{1out}} = 0.506 \\times 1 \\times 0.9975 \\times 1.0$\n",
    "\n",
    "$...$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial k_{2out}} = 0.506 \\times 1 \\times 0.9975 \\times w_{k_{2}o}$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial k_{2out}} = 0.506 \\times 1 \\times 0.9975 \\times 0.5$\n",
    "\n",
    "$...$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial k_{1out}} & \\frac{\\partial Loss}{\\partial k_{2out}}\n",
    "\\end{bmatrix} = \\left[ -0.50474\\ -0.25130 \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then find the $k_{1out}$ toward $k_{1in}$ gradient. This time, the derivative of the **sigmoid** will be use. That's already found since earlier.\n",
    "\n",
    "$k_{1out} = \\frac{1}{1 + e^{-k_{1in}}}$\n",
    "\n",
    "$\\frac{\\partial k_{1out}}{\\partial k_{1in}} = \\frac{\\partial \\left( \\frac{1}{1 + e^{-k_{1in}}} \\right)}{\\partial k_{1in}}$\n",
    "\n",
    "$\\frac{\\partial k_{1out}}{\\partial k_{1in}} = \\frac{1}{1 + e^{-k_{1in}}} \\times \\left(1 - \\frac{1}{1 + e^{-k_{1in}}} \\right)$\n",
    "\n",
    "$\\frac{\\partial k_{1out}}{\\partial k_{1in}} = \\frac{1}{1 + e^{-6}} \\times \\left(1 - \\frac{1}{1 + e^{-6}} \\right)$\n",
    "\n",
    "$...$\n",
    "\n",
    "$k_{2out} = \\frac{1}{1 + e^{-k_{2in}}}$\n",
    "\n",
    "$\\frac{\\partial k_{2out}}{\\partial k_{2in}} = \\frac{\\partial \\left( \\frac{1}{1 + e^{-k_{2in}}} \\right)}{\\partial k_{2in}}$\n",
    "\n",
    "$\\frac{\\partial k_{2out}}{\\partial k_{2in}} = \\frac{1}{1 + e^{-k_{2in}}} \\times \\left(1 - \\frac{1}{1 + e^{-k_{2in}}} \\right)$\n",
    "\n",
    "$\\frac{\\partial k_{2out}}{\\partial k_{2in}} = \\frac{1}{1 + e^{-5}} \\times \\left(1 - \\frac{1}{1 + e^{-5}} \\right)$\n",
    "\n",
    "$...$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\frac{\\partial k_{1out}}{\\partial k_{1in}} \\\\\n",
    "\\frac{\\partial k_{2out}}{\\partial k_{2in}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "0.00249 \\\\\n",
    "0.00665\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the gradient of $k_{1in}$ toward $w_{j_{1}k_{1}}$ should be searched.\n",
    "\n",
    "$k_{1in} = w_{j_{1}k_{1}}j_{1out} + w_{j_{2}k_{1}}j_{2out} + w_{j_{3}k_{1}}j_{3out} + w_{j_{4}k_{1}}j_{4out} + b_{jk_{1}}$\n",
    "\n",
    "$\\frac{\\partial k_{1in}}{\\partial w_{j_{1}k_{1}}} = \\frac{\\partial \\left(w_{j_{1}k_{1}}j_{1out} + w_{j_{2}k_{1}}j_{2out} + w_{j_{3}k_{1}}j_{3out} + w_{j_{4}k_{1}}j_{4out} + b_{jk_{1}} \\right)}{\\partial w_{j_{i}k_{1}}}$\n",
    "\n",
    "$\\begin{bmatrix} \\frac{\\partial k_{1in}}{\\partial w_{j_{1}k_{1}}} & \\frac{\\partial k_{1in}}{\\partial w_{j_{2}k_{1}}} & \\frac{\\partial k_{1in}}{\\partial w_{j_{3}k_{1}}} & \\frac{\\partial k_{1in}}{\\partial w_{j_{4}k_{1}}} \\end{bmatrix} = \\begin{bmatrix} j_{1out} & j_{2out} & j_{3out} & j_{4out} \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \\frac{\\partial k_{1in}}{\\partial w_{j_{1}k_{1}}} & \\frac{\\partial k_{1in}}{\\partial w_{j_{2}k_{1}}} & \\frac{\\partial k_{1in}}{\\partial w_{j_{3}k_{1}}} & \\frac{\\partial k_{1in}}{\\partial w_{j_{4}k_{1}}} \\end{bmatrix} = \\begin{bmatrix} 1.5 & 2.0 & 2.5 & 3.0 \\end{bmatrix}$\n",
    "\n",
    "$...$\n",
    "\n",
    "$k_{2in} = w_{j_{1}k_{2}}j_{1out} + w_{j_{2}k_{2}}j_{2out} + w_{j_{3}k_{2}}j_{3out} + w_{j_{4}k_{2}}j_{4out} + b_{jk_{2}}$\n",
    "\n",
    "$\\frac{\\partial k_{1in}}{\\partial w_{j_{1}k_{2}}} = \\frac{\\partial \\left(w_{j_{1}k_{1}}j_{1out} + w_{j_{2}k_{2}}j_{2out} + w_{j_{3}k_{2}}j_{3out} + w_{j_{4}k_{2}}j_{4out} + b_{jk_{2}} \\right)}{\\partial w_{j_{i}k_{2}}}$\n",
    "\n",
    "$\\begin{bmatrix} \\frac{\\partial k_{2in}}{\\partial w_{j_{1}k_{2}}} & \\frac{\\partial k_{2in}}{\\partial w_{j_{2}k_{2}}} & \\frac{\\partial k_{2in}}{\\partial w_{j_{3}k_{2}}} & \\frac{\\partial k_{2in}}{\\partial w_{j_{4}k_{2}}} \\end{bmatrix} = \\begin{bmatrix} j_{1out} & j_{2out} & j_{3out} & j_{4out} \\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \\frac{\\partial k_{2in}}{\\partial w_{j_{1}k_{2}}} & \\frac{\\partial k_{2in}}{\\partial w_{j_{2}k_{2}}} & \\frac{\\partial k_{2in}}{\\partial w_{j_{3}k_{2}}} & \\frac{\\partial k_{2in}}{\\partial w_{j_{4}k_{2}}} \\end{bmatrix} = \\begin{bmatrix} 1.5 & 2.0 & 2.5 & 3.0 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the **gradient loss** to $W_{j_{1}k_{1}}$ by applying the **chain rule** like the previous section.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{j_{i}k_{1}}} = \\frac{\\partial Loss}{\\partial k_{1out}} \\times \\frac{\\partial k_{1out}}{\\partial k_{1in}} \\times \\frac{\\partial k_{1in}}{\\partial w_{j_{1}k_{1}}}$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{j_{i}k_{1}}} = -0.50474 \\times \\color{red}{0.00249} \\times 1.5$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{j_{i}k_{1}}} = -0.00188$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the gradient already found, notice the red number ones. The gradient of the sigmoid is quite small at $0.00249$ and after the **Chain Rule** the result became more smaller at $-0.00188$.\n",
    "\n",
    "This phenomenon is called ***Vanishing Gradient*** and is the reason why sigmoid is rarely used anymore.\n",
    "\n",
    "The calculation that has been done earlier after applied to all parameters, will get all the gradients needed to update.\n",
    "\n",
    "The gradient in the operation above was very small (vanish), more closer a node to the input layer, more longer the time that needed to process the training, because the value of gradient that gonna used to update was very small and will be more smaller after multiplied by the learning rate.\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial w_{j_{1}k_{1}}} & \\frac{\\partial Loss}{\\partial w_{j_{1}k_{2}}} \\\\\n",
    "\\frac{\\partial Loss}{\\partial w_{j_{2}k_{1}}} & \\frac{\\partial Loss}{\\partial w_{j_{2}k_{2}}} \\\\\n",
    "\\frac{\\partial Loss}{\\partial w_{j_{3}k_{1}}} & \\frac{\\partial Loss}{\\partial w_{j_{3}k_{2}}} \\\\\n",
    "\\frac{\\partial Loss}{\\partial w_{j_{4}k_{1}}} & \\frac{\\partial Loss}{\\partial w_{j_{4}k_{2}}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "-0.00188 & -0.00252 \\\\\n",
    "-0.00251 & -0.00334 \\\\\n",
    "-0.00314 & -0.00417 \\\\\n",
    "-0.00377 & -0.00501\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial b_{jk_{1}}} & \\frac{\\partial Loss}{\\partial b_{jk_{2}}} \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-0.00125 & -0.00167\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Update for Hidden Layer 2 to Hidden Layer 1\n",
    "\n",
    "The new **weight** and **bias** will very easy to find after the gradient was found. Still using 0.25 as learning rate value to get a new **weight** and **bias**. Even the value change of **weight** and **bias** were very small, same with previous description.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "w'_{j_{1}k_{1}} & w'_{j_{1}k_{2}} \\\\\n",
    "w'_{j_{2}k_{1}} & w'_{j_{2}k_{2}} \\\\\n",
    "w'_{j_{3}k_{1}} & w'_{j_{3}k_{2}} \\\\\n",
    "w'_{j_{4}k_{1}} & w'_{j_{4}k_{2}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "w_{j_{1}k_{1}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{j_{1}k_{1}}} \\right) &\n",
    "w_{j_{1}k_{2}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{j_{1}k_{2}}} \\right) \\\\\n",
    "w_{j_{2}k_{1}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{j_{2}k_{1}}} \\right) &\n",
    "w_{j_{2}k_{2}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{j_{2}k_{2}}} \\right) \\\\\n",
    "w_{j_{3}k_{1}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{j_{3}k_{1}}} \\right) &\n",
    "w_{j_{3}k_{2}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{j_{3}k_{2}}} \\right) \\\\\n",
    "w_{j_{4}k_{1}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{j_{4}k_{1}}} \\right) &\n",
    "w_{j_{4}k_{2}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{j_{4}k_{2}}} \\right) \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1.00047 & 0.00062 \\\\\n",
    "0.75062 & 0.25083 \\\\\n",
    "0.50078 & 0.50104 \\\\\n",
    "0.25094 & 0.75125\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "b'_{jk_{1}} & b'_{jk_{1}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "b_{jk_{1}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial b_{jk_{1}}} \\right) &\n",
    "b_{jk_{2}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial b_{jk_{2}}} \\right)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "1.00031 & 1.00042\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer 1 to Input Layer (Backward Pass)\n",
    "\n",
    "<img src=\"img/bp_output_hidden_layer_1_input_layer.png\">\n",
    "\n",
    "Probably it's time do the steps that have learned. First thing is updating the **weight** and **bias** between the input layer and hidden layer 1.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{ij_{1}}} = \\frac{\\partial Loss}{\\partial j_{1out}} \\times \\frac{\\partial j_{1out}}{\\partial j_{1in}} \\times \\frac{\\partial j_{1in}}{\\partial j_{w_{ij_{1}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, search for **gradient loss** toward $j_{1out}$. This time it will be more complicated than the calculation of $k_{1out}$. Because $j_{1out}$ was influenced by a gradient that comes from $k_{2}$. So we have to see the $k$ Layer as a single unit, there's no longer $k_{1}$ and $k_{2}$.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial j_{1out}} = \\frac{\\partial Loss}{\\partial k_{out}} \\times \\frac{\\partial k_{out}}{\\partial k_{in}} \\times \\frac{\\partial k_{in}}{\\partial w_{j_{1}k}} \\times \\frac{\\partial w_{j_{1}k}}{\\partial j_{1out}}$\n",
    "\n",
    "$...$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial k_{out}} = \\frac{\\partial Loss}{\\partial k_{1out}} + \\frac{\\partial Loss}{\\partial k_{2out}} = -0.50474 + (-0.25130) = -0.75604$\n",
    "\n",
    "$\\frac{\\partial k_{out}}{\\partial k_{in}} = \\frac{\\partial k_{1out}}{\\partial k_{in}} + \\frac{\\partial k_{2out}}{\\partial k_{in}} = 0.00249 + 0.00665 = 0.00914$\n",
    "\n",
    "$\\frac{\\partial k_{in}}{\\partial w_{j_{1}k}} = \\frac{\\partial k_{1in}}{\\partial w_{j_{1}k_{1}}} + \\frac{\\partial k_{2in}}{\\partial w_{j_{1}k_{2}}} = 1.5 + 1.5 = 3.0$\n",
    "\n",
    "$\\frac{\\partial w_{j_{1}k}}{\\partial j_{1out}} = \\frac{\\partial w_{j_{1}k_{1}}}{\\partial j_{1out}} + \\frac{\\partial w_{j_{1}k_{1}}}{\\partial j_{1out}} = w_{j_{1}k_{1}} + w_{j_{1}k_{2}} =1.0 + 0 = 1.0$\n",
    "\n",
    "$...$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial j_{1out}} = \\frac{\\partial Loss}{\\partial k_{out}} \\times \\frac{\\partial k_{out}}{\\partial k_{in}} \\times \\frac{\\partial k_{in}}{\\partial w_{j_{1}k}} \\times \\frac{\\partial w_{j_{1}k}}{\\partial j_{1out}}$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial j_{1out}} = -0.75604 \\times 0.00914 \\times 3.0 \\times 1.0$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial j_{1out}} = -0.02073$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with $j_{1out}$ gradient towards $j_{1in}$.\n",
    "\n",
    "$j_{1out} = max(0, j_{1in})$\n",
    "\n",
    "$j_{1out} = max(0, 1.5)$\n",
    "\n",
    "$\\frac{\\partial j_{1out}}{\\partial j_{1in}} = \\frac{\\partial (ReLu)}{\\partial j_{1in}} = \\begin{cases}1 & j_{1in} > 0\\\\0 & j_{1in} = 0\\end{cases}$\n",
    "\n",
    "$\\frac{\\partial j_{1out}}{\\partial j_{1in}} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is find the $j_{1in}$ gradient toward $w_{ij_{1}}$.\n",
    "\n",
    "$j_{1in} = w_{ij_{1}}i + b_{ij_{1}}$\n",
    "\n",
    "$\\frac{\\partial j_{1in}}{\\partial w_{ij_{1}}} = \\frac{\\partial \\left(w_{ij_{1}}i + b_{ij_{1}} \\right)}{\\partial w_{ij_{1}}}$\n",
    "\n",
    "$\\frac{\\partial j_{1in}}{\\partial w_{ij_{1}}} = i$\n",
    "\n",
    "$\\frac{\\partial j_{1in}}{\\partial w_{ij_{1}}} = 2.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, the **gradient loss** toward $w_{ij_{1}}$ could be calculated by applied the **chain rule**.\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{ij_{1}}} = \\frac{\\partial Loss}{\\partial j_{1out}} \\times \\frac{\\partial j_{1out}}{\\partial j_{1in}} \\times \\frac{\\partial j_{1in}}{\\partial j_{w_{ij_{1}}}}$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{ij_{1}}} = -0.02073 \\times 1 \\times 2.0$\n",
    "\n",
    "$\\frac{\\partial Loss}{\\partial w_{ij_{1}}} = -0.04146$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier calculation above will be applied to all parameters. Then all gradients which need to be update has already got.\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial w_{ij_{1}}} & \n",
    "\\frac{\\partial Loss}{\\partial w_{ij_{2}}} &\n",
    "\\frac{\\partial Loss}{\\partial w_{ij_{3}}} &\n",
    "\\frac{\\partial Loss}{\\partial w_{ij_{4}}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "-0.04146 & \n",
    "-0.05528 &\n",
    "-0.06910 &\n",
    "-0.08292\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial Loss}{\\partial b_{ij_{1}}} & \n",
    "\\frac{\\partial Loss}{\\partial b_{ij_{2}}} &\n",
    "\\frac{\\partial Loss}{\\partial b_{ij_{3}}} &\n",
    "\\frac{\\partial Loss}{\\partial b_{ij_{4}}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "-0.02073 & \n",
    "-0.02764 &\n",
    "-0.03455 &\n",
    "-0.04146\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Update for Hidden Layer 1 to Input Layer\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "w'_{ij_{1}} & \n",
    "w'_{ij_{2}} &\n",
    "w'_{ij_{3}} &\n",
    "w'_{ij_{4}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "w_{ij_{1}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{ij_{1}}} \\right) & \n",
    "w_{ij_{2}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{ij_{2}}} \\right) &\n",
    "w_{ij_{3}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{ij_{3}}} \\right) &\n",
    "w_{ij_{4}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial w_{ij_{4}}} \\right)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "w'_{ij_{1}} & \n",
    "w'_{ij_{2}} &\n",
    "w'_{ij_{3}} &\n",
    "w'_{ij_{4}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "0.26037 & \n",
    "0.51382 &\n",
    "0.76728 &\n",
    "1.02073\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "b'_{ij_{1}} & \n",
    "b'_{ij_{2}} &\n",
    "b'_{ij_{3}} &\n",
    "b'_{ij_{4}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "b_{ij_{1}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial b_{ij_{1}}} \\right) & \n",
    "b_{ij_{2}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial b_{ij_{2}}} \\right) &\n",
    "b_{ij_{3}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial b_{ij_{3}}} \\right) &\n",
    "b_{ij_{4}} - \\alpha \\left(\\frac{\\partial Loss}{\\partial b_{ij_{4}}} \\right)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\begin{bmatrix} \n",
    "b'_{ij_{1}} & \n",
    "b'_{ij_{2}} &\n",
    "b'_{ij_{3}} &\n",
    "b'_{ij_{4}}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "1.02073 & \n",
    "1.02764 &\n",
    "1.03455 &\n",
    "1.04146\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Finally finished. The all the new parameters has been found. This process (**Forward Pass** and **Backward Pass**) will be repeated continuously until the smallest loss value reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Parameter vs New Parameter\n",
    "\n",
    "- **Weight**\n",
    "\n",
    "$W_{jk} = \\begin{bmatrix}w_{ij_{1}} & w_{ij_{2}} & w_{ij_{3}} & w_{ij_{4}} \\end{bmatrix} = \\begin{bmatrix}0.25 & 0.5 & 0.75 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$W_{jk} = \\begin{bmatrix}\n",
    "w_{j_{1}k_{1}} & w_{j_{1}k_{2}} \\\\\n",
    "w_{j_{2}k_{1}} & w_{j_{1}k_{2}} \\\\\n",
    "w_{j_{3}k_{1}} & w_{j_{3}k_{2}} \\\\\n",
    "w_{j_{4}k_{1}} & w_{j_{4}k_{2}}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1.0 & 0 \\\\\n",
    "0.75 & 0.25 \\\\\n",
    "0.5 & 0.5 \\\\\n",
    "0.25 & 0.75\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$W_{ko} = \\begin{bmatrix}w_{k_{1}o} \\\\ w_{k_{2}o} \\end{bmatrix} = \\begin{bmatrix}1.0 \\\\ 0.5 \\end{bmatrix}$\n",
    "\n",
    "- **Bias**\n",
    "\n",
    "$b_{ij} = \\begin{bmatrix}b_{ij_{1}} & b_{ij_{2}} & b_{ij_{3}} & b_{ij_{4}} \\end{bmatrix} = \\begin{bmatrix} 1.0 & 1.0 & 1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$b_{jk} = \\begin{bmatrix}b_{jk_{1}} & b_{jk_{2}} \\end{bmatrix} = \\begin{bmatrix} 1.0 & 1.0 \\end{bmatrix}$\n",
    "\n",
    "$b_{o} = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n",
    "\n",
    "$...$\n",
    "\n",
    "- **Weight**\n",
    "\n",
    "$W'_{jk} = \\begin{bmatrix}w_{ij_{1}} & w_{ij_{2}} & w_{ij_{3}} & w_{ij_{4}} \\end{bmatrix} = \\begin{bmatrix}0.26037 & 0.51382 & 0.76728 & 1.02073 \\end{bmatrix}$\n",
    "\n",
    "$W'_{jk} = \\begin{bmatrix}\n",
    "w_{j_{1}k_{1}} & w_{j_{1}k_{2}} \\\\\n",
    "w_{j_{2}k_{1}} & w_{j_{1}k_{2}} \\\\\n",
    "w_{j_{3}k_{1}} & w_{j_{3}k_{2}} \\\\\n",
    "w_{j_{4}k_{1}} & w_{j_{4}k_{2}}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1.00047 & 0.00062 \\\\\n",
    "0.75062 & 0.25083 \\\\\n",
    "0.50078 & 0.50104 \\\\\n",
    "0.25094 & 0.75125\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$W'_{ko} = \\begin{bmatrix}w_{k_{1}o} \\\\ w_{k_{2}o} \\end{bmatrix} = \\begin{bmatrix}1.1262 \\\\ 0.6256 \\end{bmatrix}$\n",
    "\n",
    "- **Bias**\n",
    "\n",
    "$b'_{ij} = \\begin{bmatrix}b_{ij_{1}} & b_{ij_{2}} & b_{ij_{3}} & b_{ij_{4}} \\end{bmatrix} = \\begin{bmatrix} 1.02073 & 1.02674 & 1.03455 & 1.04146 \\end{bmatrix}$\n",
    "\n",
    "$b'_{jk} = \\begin{bmatrix}b_{jk_{1}} & b_{jk_{2}} \\end{bmatrix} = \\begin{bmatrix} 1.00031 & 1.00042 \\end{bmatrix}$\n",
    "\n",
    "$b'_{o} = \\begin{bmatrix} 1.1265 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the example above only used one data at **Forward** and **Backward Pass**. In general, the **Gradient Descent** consists of 3 types, the **SGD** that used above, the **Batch Gradient Descent** and the **Mini-batch Gradient Descent**.\n",
    "\n",
    "In the **Batch Gradient Descent (BGD)**, the model would be updated after all data has been \"propagated\". Whereas the Mini-batch is in the middle of SGD and BGD.\n",
    "\n",
    "**Mini-batch gradient descent** does **Forwards** and **Backward Pass** on a small group of training data. For example doing an update for every 32/64 pieces of data and calculated the error was the mean from a group of training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
