{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "Look at the architecture of neural network in this image. It contains 3 input layer nodes, 4 hidden layer nodes, and 2 output layer nodes. Of course it will be have $(3 \\times 4)$ number of weight (W) and 4 biases (b) same numbers with the hidden layer.\n",
    "\n",
    "<img src=\"img/nn_single_layer.png\">\n",
    "\n",
    "The main part of neural network part:\n",
    "\n",
    "- **Input layer**: The main input of the neural network, represented as node and the number of it depends with the data to process.\n",
    "- **Hidden layer**: The layer to process the input, usually in the middle between Input layer and output layer. Represented as nodes, and the number of it customizeable depends with the user want.\n",
    "- **Ouput layer**: The layer to show or collect the output, represented as nodes, and the number of it customizeable depends with result or the model that we want.\n",
    "- **Weight**: The value which always updated during the communication between the Input layer nodes and the Hidden layer nodes, so the numbers of weight (W) is `number of input layer nodes` $\\times$ `number of hidden layer nodes`.\n",
    "- **Bias**: The bias (b) is additional value to add after input pass the hidden layer, the number of bias (b) is depend with the number of hidden layers.\n",
    "- **Activation function**: Like it's name when compared with the neuron system, it used to chose which the neuron to activate. When implemented in this scheme, it means which the node of hidden layer to activate. So, the number of activation function depends with the number of hidden layer nodes and output nodes (`number of hidden layer nodes` $\\times$ `number of output layer nodes`). There are a lot of activation function method such as linear function, sigmoid, tanh, softmax and etc.\n",
    "\n",
    "## Learning Process\n",
    "\n",
    "In neural network, Here's some learning process that usually used:\n",
    "\n",
    "- Training\n",
    "- Evaluation\n",
    "- Testing (Optional)\n",
    "\n",
    "During **The training**, the Weight $W$ and bias $b$ in each neuron or node will be update until the expected result reached. While the training process running, **The evaluation** are running too. It will make sure if the expected result reached yet, and the training process will be stopped. The tesing process is about how implemented the model as the result of the training and the evaluation in the new data (data testing).\n",
    "\n",
    "For this section will only mention two training step, the two training step is:\n",
    "\n",
    "- Forward Pass\n",
    "- Backward Pass\n",
    "\n",
    "**Forward pass** or also called forward propagation is a process where the dataa carried from the input through the each neuron in the hidden layer to the output layer which the error will be calculate later.\n",
    "\n",
    "$y_{j} = \\sum_{i = 10}^{M} W_{ji}x_{i} + b_{j}$\n",
    "\n",
    "$h_{j} = \\theta(y_{j}) = max(0, y_{j})$\n",
    "\n",
    "The equation above is an example of a forward pass on the first architecture (see architectural image above) that uses ReLU as an activation function. Where $i$ is the node at the $M$ input layer (3 input nodes from 0-2), $j$ is the node at the hidden layer while $h$ is the output of the node at the hidden layer.\n",
    "\n",
    "**Backward pass** is the error that was got on the **forward pass** and will be used to update each weight $W$ and bias $b$ with a certain learning rate.\n",
    "\n",
    "The two processes above will be repeated until a weight $W$ and bias $b$ value is obtained, which it can give the smallest possible error value to the output layer (during the forward pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, The implementation of forward pass was tried using Python and Numpy without a framework to make it clearer. Later in the next parts we will try with Tensorflow and Keras.\n",
    "\n",
    "## Problem\n",
    "\n",
    "Solve the regression problem, the actually equation is like this.\n",
    "\n",
    "$f(x) = 3x + 2$\n",
    "\n",
    "Get the closest model like that equation.\n",
    "\n",
    "$f(x) = \\theta_{0}x + \\theta_{1}$\n",
    "\n",
    "It means get the closest $\\theta$ value with the actual equation. While the neural network architecture consists of:\n",
    "- 1 node on the input layer $\\rightarrow$ $(x)$\n",
    "- 1 node on the output layer $\\rightarrow$ $f(x)$\n",
    "\n",
    "The neural network have been trained and The forward pass will be used to update the weight and bias thar had been obtained during the training before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "The forward pass method below is very simple, `dot` operations will be performed on each element in the input and each weight $W$ that is connected to the input and added with bias $b$. The results of this operation will be entered into the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Linear Activation f(x)\n",
    "def forward_pass(inputs, weight, bias):\n",
    "    w_sum = np.dot(inputs, weight) + bias\n",
    "    \n",
    "    return w_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Weight\n",
    "\n",
    "For the weight and bias that we will try, both values have been obtained in the training process that have been done before. How to get the two values will be explained in the following parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:  [[2.99999928]]\n",
      "Bias:  [1.99999976]\n"
     ]
    }
   ],
   "source": [
    "# Pre-Trained Weights & Biases after Training\n",
    "W = np.array([[2.99999928]])\n",
    "b = np.array([1.99999976])\n",
    "\n",
    "print(\"Weight: \", W)\n",
    "print(\"Bias: \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Input Data\n",
    "inputs = np.array([[7], [8], [9], [10]])\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation process of the inputs and the weight $W$ represented with matrix, the actually is the input layer is only contain one neuron. But in this case, every matrix row represented different input $x$ to train.\n",
    "\n",
    "$f(x) = W x + b$\n",
    "\n",
    "which the value of weight and bias has been trained, so the **Forward Pass** will be like this:\n",
    "\n",
    "$f(x) = [2.99999928] \\times \\begin{bmatrix}7 \\\\8 \\\\9 \\\\10 \\end{bmatrix} + [1.99999976]$\n",
    "\n",
    "The trained value must be not really far compared with the main equations like this explanation:\n",
    "\n",
    "$f(x) = 3x + 2 \\approx 2.99999928x + 1.99999976$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Layer Output (Linear)\n",
      "============================\n",
      "[[22.99999472]\n",
      " [25.999994  ]\n",
      " [28.99999328]\n",
      " [31.99999256]]\n"
     ]
    }
   ],
   "source": [
    "# Output of Output Layer\n",
    "o_out = forward_pass(inputs, W, b)\n",
    "\n",
    "print('Output Layer Output (Linear)')\n",
    "print('============================')\n",
    "print(o_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will make a prediction of values of 7, 8, 9 and 10. The resulting output should be 23, 26, 29, 32 and the predicted results are 22.99999472, 25.999994, 28.99999328 and 31.99999256. When viewed from the results of predictions, there are still errors but with very small values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
