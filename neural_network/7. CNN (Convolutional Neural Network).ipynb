{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "<img src=\"img/cnn_mnist.png\">\n",
    "\n",
    "The MNIST data above consists of values from 0 to 255 for each pixel present. So, MLP  can be use to classify all digits with pretty good results because most of the data in MNIST, the object to be recognized is in the middle of the image.\n",
    "\n",
    "Then what if the object to be recognized was not in the middle of the image? This is the weakness of MLP. The number 6 in the middle of the picture will be recognized, but the number 6 in the left corner may not be recognized.\n",
    "\n",
    "But a lot of data with each digit in a different location can be use, but this is not an efficient way to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "Convolutional Neural Network (CNN) is a type of neural network commonly used in image data. CNN can be used to detect and recognize objects in an image. Broadly speaking, CNN is not much different from the usual neural network. CNN consists of neurons that have a weight, bias and activation function.\n",
    "\n",
    "Then what is the difference? The architecture of CNN is divided into 2 major parts, **Feature Lerning Layer** and **Fully-Connected Layer** or **Multi-Layer Perceptron**.\n",
    "\n",
    "<img src=\"img/cnn_architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Learning Layer\n",
    "\n",
    "That term was used because the process that occurs in this section was \"encoding\" from an image into features in the form of numbers that represent the image (Feature Extraction). Feature Learning layer consists of two parts. **Convolutional Layer** and **Pooling Layer**. But sometimes there are some research/papers that don't use pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer (Conv. Layer)\n",
    "\n",
    "<img src=\"img/cnn_conv_layer.png\">\n",
    "\n",
    "The picture above is an RGB (Red, Green, Blue) **image measuring 32x32 pixels** which is actually a multidimensional array with a size of **32x32x3 (3 is the number of channels)**.\n",
    "\n",
    "Convolutional layer consists of neurons arranged in such a way as to form a filter with length and height (pixels). For example, the first layer in the feature learning layer is usually convolutional layer with a size of the image is 5x5x3. **5 pixels length**, **5 pixels height** and **3 pieces depth/total** according to the channel of the image.\n",
    "\n",
    "These three filters will be shifted to all parts of the image. Every shift will be carried out a \"dot\" operation between the input and the value of the filter to produce an output or commonly referred to as an activation map or feature map.\n",
    "\n",
    "<img src=\"img/cnn_conv_process.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "Stride is a parameter that determines how many filters shift. **If the value of stride is 1, then convolution filter will shift 1 pixel horizontally then vertically**. In the illustration above, the stride that used was 2.\n",
    "\n",
    "More smaller the stride, the more detailed information that got from the input, but it requires more computing performance when compared to a large stride. However, it should be noted, even using a small stride the performance won't always good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "Padding or Zero Padding is a parameter that **determines the number of pixels (containing the value 0) to be added on each side of the input images**. This is used for the purpose of manipulating the output dimensions of convolution layer (Feature Map).\n",
    "\n",
    "The purpose of using padding is:\n",
    "\n",
    "- **Output dimension of convolution layer is always smaller than the input (except the use of a 1x1 filter with stride 1)**. This output will be reused as input from next convolution layer, so that more and more information is wasted.\n",
    "\n",
    "    By using padding, the output dimensions can be adjusted same as the input dimensions or at least not decrease drastically. So, we can use deeper convolution layer/deep convolution layer, that more features were successfully extracted.\n",
    "\n",
    "- Improve the performance of the model because of the convolution filter will focus on the actual information that exist between the zero padding.\n",
    "\n",
    "In the illustration above, the dimensions of **the actual input was 5x5**, if convolution is done with **3x3 filter and 2 stride**, **you will get only 2x2 feature map**. But **if 1 zero padding was added, the feature map would be 3x3 in size** (more information will be generated).\n",
    "\n",
    "Here's the equation to calculate the dimensions of the feature map:\n",
    "\n",
    "$output = \\frac{W\\ -\\ N\\ +\\ 2P}{S} + 1$\n",
    "\n",
    "- W = Length/Height of Input\n",
    "- N = Filter Length/Height\n",
    "- P = Zero Padding\n",
    "- S = Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layer\n",
    "\n",
    "The pooling layer is usually exist after convolution layer. In principle, the pooling layer consists of a filter with certain size and stride that will shift throughout the feature map area.\n",
    "\n",
    "Pooling commonly used Max Pooling and Average Pooling. For example Max Pooling 2x2 was used with stride 2, then at each shift of the filter, the maximum value in the 2x2 pixel area will be selected, while Average Pooling will choose the average value.\n",
    "\n",
    "<img src=\"img/cnn_pooling.png\">\n",
    "\n",
    "*The output dimension of the Pooling layer also uses the same formula as conv. layer.*\n",
    "\n",
    "The purpose of using the pooling layer is to reduce the dimensions of the feature map (downsampling), thereby speeding up the computation because the parameters that need to be updated are fewer and overcome overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Layer (FC Layer)\n",
    "\n",
    "The feature map that is produced from the feature extraction layer is **still in the form of a multidimensional array**. So, **it need to be \"flatten\" or reshaped the feature map into a vector**. So, it can be used as input for the fully-connected layer.\n",
    "\n",
    "FC Layer is meant here is the MLP that was studied in part-4 and part-5 before. FC Layer has **several hidden layers, activation functions, output layers and loss functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the detail about backpropagation process in CNN: [https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/](https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Code\n",
    "\n",
    "This time MNIST Fashion data must be classify. This MNIST Fashion is a dataset consisting of 10 fashion categories as follows:\n",
    "\n",
    "<img src=\"img/fashion_mnist.png\">\n",
    "\n",
    "+ T-Shirt/Tops = 0\n",
    "+ Trouser = 1\n",
    "+ Pullover = 2\n",
    "+ Dress = 3\n",
    "+ Coat = 4\n",
    "+ Sandal = 5\n",
    "+ Shirt = 6\n",
    "+ Sneaker = 7\n",
    "+ Bag = 8\n",
    "+ Ankle Boot = 9\n",
    "\n",
    "Each category consists of 6,000 images for training and 1,000 images for testing. So the total for training data is 60,000 images and 10,000 for testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Activation, Dense, Conv2D, MaxPooling2D, ZeroPadding2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n",
    "\n",
    "train_x = train_x.astype('float32') / 255.\n",
    "test_x = test_x.astype('float32') / 255.\n",
    "\n",
    "train_x = np.reshape(train_x, (len(train_x), 28, 28, 1))\n",
    "test_x = np.reshape(test_x, (len(test_x), 28, 28, 1))\n",
    "\n",
    "train_y = to_categorical( train_y )\n",
    "test_y = to_categorical( test_y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really different from the previous parts. But this time the new layers named **Conv2D**, **MaxPooling2D**, **ZeroPadding2D** and **Flatten** will be used. **TensorBoard** will also be used to *visualize during training*.\n",
    "\n",
    "**MNIST Fashion** Input will be scaled from 0 - 255 to 0 - 1 same with the explanation in part-6 and reshape the data to 4-D because the requirements of the framework that will be used was like the code above *(batch_size*, *width*, *height*, *channel)* $\\rightarrow$ (256, 28, 28, 1).\n",
    "\n",
    "The targets will also be changed to **one-hot** by using the *to_categorical* method same as what did in part-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "<img src=\"img/cnn_model.jpg\">\n",
    "\n",
    "The architectural model that gonna be make is like the picture above. Feature map that was successfully extracted from the input size of 64x3. Furthermore, there is a Flatten layer that changes the feature map to 1-D vector which will be used on the FC Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 16)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 188,362\n",
      "Trainable params: 188,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(28, 28, 1))\n",
    "conv_layer = ZeroPadding2D(padding=(2,2))(inputs) \n",
    "conv_layer = Conv2D(16, (5, 5), strides=(3,3), activation='relu')(conv_layer) \n",
    "conv_layer = MaxPooling2D((2, 2))(conv_layer)\n",
    "conv_layer = Conv2D(32, (3, 3), strides=(1,1), activation='relu')(conv_layer) \n",
    "conv_layer = ZeroPadding2D(padding=(1,1))(conv_layer) \n",
    "conv_layer = Conv2D(64, (3, 3), strides=(1,1), activation='relu')(conv_layer)\n",
    "\n",
    "# Flatten feature map to Vector with 576 element.\n",
    "flatten = Flatten()(conv_layer) \n",
    "\n",
    "# Fully Connected Layer\n",
    "fc_layer = Dense(256, activation='relu')(flatten)\n",
    "fc_layer = Dense(64, activation='relu')(fc_layer)\n",
    "outputs = Dense(10, activation='softmax')(fc_layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Adam Optimizer and Cross Entropy Loss\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print Model Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with TensorBoard Visualization\n",
    "\n",
    "TensorBoard will be used to visualize during the training. All training **loss/accuracy** and **validation loss/accuracy** will be saved and the graph can be seen. So, the result can be seen, whether there is **underfit**, **overfit** and **the performance of our model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 1.3813 - accuracy: 0.5990 - val_loss: 0.7665 - val_accuracy: 0.7225\n",
      "WARNING:tensorflow:From /home/rohwid/PyEnvironment/tf-1.15-all-package-py.3.6/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.6856 - accuracy: 0.7488 - val_loss: 0.6608 - val_accuracy: 0.7551\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.6186 - accuracy: 0.7716 - val_loss: 0.6197 - val_accuracy: 0.7733\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.5798 - accuracy: 0.7874 - val_loss: 0.5918 - val_accuracy: 0.7866\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.5534 - accuracy: 0.7994 - val_loss: 0.5628 - val_accuracy: 0.7959\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.5305 - accuracy: 0.8085 - val_loss: 0.5354 - val_accuracy: 0.8064\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 7s 108us/step - loss: 0.5105 - accuracy: 0.8174 - val_loss: 0.5215 - val_accuracy: 0.8086\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.4939 - accuracy: 0.8248 - val_loss: 0.5161 - val_accuracy: 0.8139\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4784 - accuracy: 0.8304 - val_loss: 0.4881 - val_accuracy: 0.8255\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.4657 - accuracy: 0.8357 - val_loss: 0.4747 - val_accuracy: 0.8325\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.4537 - accuracy: 0.8394 - val_loss: 0.4722 - val_accuracy: 0.8356\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.4433 - accuracy: 0.8428 - val_loss: 0.4613 - val_accuracy: 0.8365\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.4326 - accuracy: 0.8473 - val_loss: 0.4569 - val_accuracy: 0.8398\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.4256 - accuracy: 0.8491 - val_loss: 0.4416 - val_accuracy: 0.8418\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.4199 - accuracy: 0.8504 - val_loss: 0.4400 - val_accuracy: 0.8469\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.4085 - accuracy: 0.8551 - val_loss: 0.4290 - val_accuracy: 0.8513\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.4027 - accuracy: 0.8562 - val_loss: 0.4255 - val_accuracy: 0.8503\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.3966 - accuracy: 0.8588 - val_loss: 0.4156 - val_accuracy: 0.8549\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3934 - accuracy: 0.8589 - val_loss: 0.4107 - val_accuracy: 0.8546\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.3861 - accuracy: 0.8623 - val_loss: 0.4081 - val_accuracy: 0.8557\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3805 - accuracy: 0.8643 - val_loss: 0.3989 - val_accuracy: 0.8574\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.3770 - accuracy: 0.8660 - val_loss: 0.3934 - val_accuracy: 0.8598\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.3715 - accuracy: 0.8674 - val_loss: 0.3938 - val_accuracy: 0.8620\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.3675 - accuracy: 0.8673 - val_loss: 0.3905 - val_accuracy: 0.8615\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.3653 - accuracy: 0.8686 - val_loss: 0.3893 - val_accuracy: 0.8618\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.3604 - accuracy: 0.8709 - val_loss: 0.3860 - val_accuracy: 0.8627\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.3551 - accuracy: 0.8723 - val_loss: 0.3834 - val_accuracy: 0.8596\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.3536 - accuracy: 0.8734 - val_loss: 0.3996 - val_accuracy: 0.8542\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.3504 - accuracy: 0.8741 - val_loss: 0.3830 - val_accuracy: 0.8612\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.3465 - accuracy: 0.8755 - val_loss: 0.3808 - val_accuracy: 0.8621\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.3429 - accuracy: 0.8769 - val_loss: 0.3766 - val_accuracy: 0.8643\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.3402 - accuracy: 0.8777 - val_loss: 0.3674 - val_accuracy: 0.8690\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.3383 - accuracy: 0.8780 - val_loss: 0.3636 - val_accuracy: 0.8679\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.3347 - accuracy: 0.8799 - val_loss: 0.3672 - val_accuracy: 0.8684\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3341 - accuracy: 0.8802 - val_loss: 0.3599 - val_accuracy: 0.8693\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.3293 - accuracy: 0.8816 - val_loss: 0.3552 - val_accuracy: 0.8710\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.3270 - accuracy: 0.8817 - val_loss: 0.3694 - val_accuracy: 0.8649\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.3263 - accuracy: 0.8820 - val_loss: 0.3568 - val_accuracy: 0.8717\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3218 - accuracy: 0.8841 - val_loss: 0.3553 - val_accuracy: 0.8708\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3193 - accuracy: 0.8845 - val_loss: 0.3566 - val_accuracy: 0.8708\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3184 - accuracy: 0.8858 - val_loss: 0.3559 - val_accuracy: 0.8733\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3179 - accuracy: 0.8857 - val_loss: 0.3487 - val_accuracy: 0.8749\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3137 - accuracy: 0.8873 - val_loss: 0.3470 - val_accuracy: 0.8744\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3100 - accuracy: 0.8887 - val_loss: 0.3463 - val_accuracy: 0.8746\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3081 - accuracy: 0.8894 - val_loss: 0.3488 - val_accuracy: 0.8737\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.3058 - accuracy: 0.8892 - val_loss: 0.3453 - val_accuracy: 0.8749\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3056 - accuracy: 0.8900 - val_loss: 0.3407 - val_accuracy: 0.8780\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3041 - accuracy: 0.8899 - val_loss: 0.3378 - val_accuracy: 0.8810\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.3000 - accuracy: 0.8916 - val_loss: 0.3347 - val_accuracy: 0.8793\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2998 - accuracy: 0.8924 - val_loss: 0.3352 - val_accuracy: 0.8781\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2975 - accuracy: 0.8916 - val_loss: 0.3315 - val_accuracy: 0.8805\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2944 - accuracy: 0.8931 - val_loss: 0.3354 - val_accuracy: 0.8779\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2936 - accuracy: 0.8935 - val_loss: 0.3382 - val_accuracy: 0.8743\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2927 - accuracy: 0.8927 - val_loss: 0.3272 - val_accuracy: 0.8826\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2887 - accuracy: 0.8949 - val_loss: 0.3279 - val_accuracy: 0.8798\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2876 - accuracy: 0.8960 - val_loss: 0.3268 - val_accuracy: 0.8840\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2866 - accuracy: 0.8958 - val_loss: 0.3290 - val_accuracy: 0.8797\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2855 - accuracy: 0.8970 - val_loss: 0.3404 - val_accuracy: 0.8754\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2834 - accuracy: 0.8972 - val_loss: 0.3380 - val_accuracy: 0.8763\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2815 - accuracy: 0.8976 - val_loss: 0.3262 - val_accuracy: 0.8805\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2795 - accuracy: 0.8992 - val_loss: 0.3298 - val_accuracy: 0.8800\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2773 - accuracy: 0.8994 - val_loss: 0.3191 - val_accuracy: 0.8860\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2764 - accuracy: 0.8993 - val_loss: 0.3187 - val_accuracy: 0.8859\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2738 - accuracy: 0.9007 - val_loss: 0.3266 - val_accuracy: 0.8834\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2743 - accuracy: 0.9010 - val_loss: 0.3226 - val_accuracy: 0.8850\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2722 - accuracy: 0.9010 - val_loss: 0.3168 - val_accuracy: 0.8879\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2690 - accuracy: 0.9022 - val_loss: 0.3196 - val_accuracy: 0.8865\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2682 - accuracy: 0.9025 - val_loss: 0.3209 - val_accuracy: 0.8836\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.2680 - accuracy: 0.9029 - val_loss: 0.3196 - val_accuracy: 0.8861\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2668 - accuracy: 0.9032 - val_loss: 0.3148 - val_accuracy: 0.8873\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2655 - accuracy: 0.9036 - val_loss: 0.3130 - val_accuracy: 0.8897\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2628 - accuracy: 0.9036 - val_loss: 0.3113 - val_accuracy: 0.8906\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2624 - accuracy: 0.9049 - val_loss: 0.3174 - val_accuracy: 0.8859\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2619 - accuracy: 0.9039 - val_loss: 0.3136 - val_accuracy: 0.8844\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.2614 - accuracy: 0.9042 - val_loss: 0.3132 - val_accuracy: 0.8886\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.2575 - accuracy: 0.9064 - val_loss: 0.3196 - val_accuracy: 0.8847\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2573 - accuracy: 0.9065 - val_loss: 0.3174 - val_accuracy: 0.8834\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2548 - accuracy: 0.9069 - val_loss: 0.3111 - val_accuracy: 0.8879\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2555 - accuracy: 0.9067 - val_loss: 0.3131 - val_accuracy: 0.8845\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2532 - accuracy: 0.9073 - val_loss: 0.3130 - val_accuracy: 0.8871\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2525 - accuracy: 0.9077 - val_loss: 0.3067 - val_accuracy: 0.8894\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2488 - accuracy: 0.9099 - val_loss: 0.3138 - val_accuracy: 0.8876\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2508 - accuracy: 0.9087 - val_loss: 0.3073 - val_accuracy: 0.8899\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2489 - accuracy: 0.9089 - val_loss: 0.3042 - val_accuracy: 0.8888\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2463 - accuracy: 0.9107 - val_loss: 0.3146 - val_accuracy: 0.8879\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2468 - accuracy: 0.9090 - val_loss: 0.3040 - val_accuracy: 0.8893\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2458 - accuracy: 0.9100 - val_loss: 0.2994 - val_accuracy: 0.8922\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2440 - accuracy: 0.9104 - val_loss: 0.3041 - val_accuracy: 0.8910\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2426 - accuracy: 0.9103 - val_loss: 0.3060 - val_accuracy: 0.8871\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2395 - accuracy: 0.9122 - val_loss: 0.3041 - val_accuracy: 0.8916\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2395 - accuracy: 0.9122 - val_loss: 0.3015 - val_accuracy: 0.8878\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2380 - accuracy: 0.9124 - val_loss: 0.3060 - val_accuracy: 0.8918\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2388 - accuracy: 0.9125 - val_loss: 0.2999 - val_accuracy: 0.8901\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.2356 - accuracy: 0.9142 - val_loss: 0.3002 - val_accuracy: 0.8907\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2363 - accuracy: 0.9140 - val_loss: 0.2971 - val_accuracy: 0.8926\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2359 - accuracy: 0.9140 - val_loss: 0.2980 - val_accuracy: 0.8920\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.2340 - accuracy: 0.9140 - val_loss: 0.2984 - val_accuracy: 0.8924\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2329 - accuracy: 0.9152 - val_loss: 0.2980 - val_accuracy: 0.8923\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2319 - accuracy: 0.9144 - val_loss: 0.3013 - val_accuracy: 0.8909\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.2318 - accuracy: 0.9147 - val_loss: 0.2958 - val_accuracy: 0.8921\n"
     ]
    }
   ],
   "source": [
    "# Use TensorBoard\n",
    "callbacks = TensorBoard(log_dir='./Graph')\n",
    "\n",
    "# Train for 100 Epochs and use TensorBoard Callback\n",
    "model.fit(train_x, train_y, batch_size=256, epochs=100, verbose=1, \n",
    "          validation_data=(test_x, test_y), callbacks=[callbacks])\n",
    "\n",
    "# Save Weights\n",
    "model.save_weights('7_conv_neural_network.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 100 epochs, amount of loss is 0.2236 for training and 0.3071 for validation, while the accuracy obtained is 0.9186 (91.86%) for training and 0.8891 (88.9%) for validation.\n",
    "\n",
    "To use TensorBoard, this command can be following:\n",
    "\n",
    "`$ tensorboard --logdir=Graph/`\n",
    "\n",
    "Here's the result which has shown in the tensorboard.\n",
    "\n",
    "<img src=\"img/tf_board_cnn.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
